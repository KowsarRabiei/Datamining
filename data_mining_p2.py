# -*- coding: utf-8 -*-
"""data_mining_p2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YJaPiyKfEQEQw_kYnZXRfVI_tWQb_APm
"""

import pandas as pd #to read csv file and work with dataframe
import numpy as np
import matplotlib.pyplot as plt
from itertools import combinations
from itertools import permutations

df = pd.read_csv("groceries.csv", header=None)
df

df.info

items_per_order=list(df.count())
df.count()

l=list(range(1,33))

plt.bar(l,items_per_order)

plt.hist(items_per_order)

type_list=[]
for i in list(range(0,32)):
    type_list = type_list+df[i].unique().tolist()

items=list(np.unique(type_list))
len(items) #NUMBER OF TOTAL UNIQUE ITEMS IN THE GROCERY STORE

TOTAL_list=[]
for i in list(range(0,32)):
    TOTAL_list = TOTAL_list+df[i].tolist()

unique, counts = np.unique(TOTAL_list, return_counts=True)
myDictionary = dict(zip(unique, counts))
plt.figure(figsize =(50, 7))
plt.xticks(rotation='vertical')
plt.bar(myDictionary.keys(), myDictionary.values(), color='g')

pd.set_option('display.max_columns', None)
dummies=pd.get_dummies(df)
dummies

DataF = pd.DataFrame()

for col in items:
    DataF[col] = dummies[[c for c in dummies.columns if col in c]].sum(axis=1)
DF=DataF

DF["sum"] = DF.sum(axis=1)
DF #ADDING A COLUMN IN THE END OF TABLE SUMMING OVER ENTIRE ROW EQUALS TO NUMBER OF ITEMS IN EACH ORDER

DF.loc['Column_Total']= DF.sum(numeric_only=True, axis=0)
DF #ADDING A ROW IN THE END OF TABLE SUMMING OVER ENTIRE COLUMNS EQUALS TO TOTAL NUMBER OF ORDERED ITEMS FOR EACH ITEM

TOTAL_SALES_PER_ITEM=DF.iloc[-1][0:170]
TOTAL_SALES_PER_ITEM

TOTAL_SALES_PER_ITEM.hist(bins=50)

TOTAL_SALES=DF.iloc[-1][170]
TOTAL_SALES

plt.figure(figsize =(50, 7))
plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[0:170],TOTAL_SALES_PER_ITEM.tolist()[0:170])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[0:10],TOTAL_SALES_PER_ITEM.tolist()[0:10])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[10:20],TOTAL_SALES_PER_ITEM.tolist()[10:20])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[20:30],TOTAL_SALES_PER_ITEM.tolist()[20:30])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[30:40],TOTAL_SALES_PER_ITEM.tolist()[30:40])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[40:50],TOTAL_SALES_PER_ITEM.tolist()[40:50])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[50:60],TOTAL_SALES_PER_ITEM.tolist()[50:60])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[60:70],TOTAL_SALES_PER_ITEM.tolist()[60:70])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[70:80],TOTAL_SALES_PER_ITEM.tolist()[70:80])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[80:90],TOTAL_SALES_PER_ITEM.tolist()[80:90])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[90:100],TOTAL_SALES_PER_ITEM.tolist()[90:100])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[100:110],TOTAL_SALES_PER_ITEM.tolist()[100:110])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[110:120],TOTAL_SALES_PER_ITEM.tolist()[110:120])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[120:130],TOTAL_SALES_PER_ITEM.tolist()[120:130])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[130:140],TOTAL_SALES_PER_ITEM.tolist()[130:140])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[140:150],TOTAL_SALES_PER_ITEM.tolist()[140:150])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[150:160],TOTAL_SALES_PER_ITEM.tolist()[150:160])

plt.xticks(rotation='vertical')
plt.bar(DF.columns.values.tolist()[160:170],TOTAL_SALES_PER_ITEM.tolist()[160:170])

dataframe=DF[:-1]
dataframe

dataf=dataframe.drop(dataframe.columns[-1],axis=1)
dataf

plt.figure(figsize =(50, 7))
plt.xticks(rotation='vertical')
dataf.boxplot()

"""# for first couple of items"""

boxplot = dataf.boxplot(column=['Instant food products','UHT-milk','abrasive cleaner','artif. sweetener','baby cosmetics','baby food','bags'])

"""# ploting boxplot for the total sold items"""

plt.boxplot(TOTAL_SALES_PER_ITEM.tolist())

"""# ploting box plot to see how many items are in each order"""

plt.boxplot(dataframe['sum'])

DF

Df=DF.drop(dataframe.columns[-1],axis=1)
Df

"""# as you can see in the table below, items are sorted by the number they have been sold. so the first column is the least selling item which is sound storage medium as well as baby food with only one order and the last one is the most selling item which is whole milk."""

rslt_df = Df[-1:].sort_values(by = 'Column_Total', axis = 1)
rslt_df

"""# follwong code shows that 36 is the maximum number of items in one order which is related to order #2973 and the next one get the minimum number which in one"""

maxitems = dataframe['sum'].max()
maxitems

dataframe[dataframe['sum'] == maxitems]

"""# there are 1892 orders with only one item"""

minitems = dataframe['sum'].min()
minitems

dataframe[dataframe['sum'] == minitems]

"""# finding median"""

medianitem=dataframe['sum']. median()
medianitem

dataframe[dataframe['sum'] == medianitem]

"""# finding mean"""

meanitem=dataframe['sum']. mean()
meanitem

rslt_df

"""# step 2
## Apriori
"""

def get_frequent_item_sets(data, min_sup):
  item_set=set(data.columns)
  support_dic={}
  l=len(set(data.columns))
  for i in range(1,l+1):
    c=combinations(item_set,i)
    item_set=set()
    for j in list(c):
      Sup=data.loc[:,j].product(axis=1).sum()/len(data.index)
      if Sup> min_sup:
        support_dic[j]=Sup
        item_set=set(set(item_set) | set(j))
  result=pd.DataFrame(list(support_dic.items()),columns=["items","support"])
  return(result)

"""# step 3"""

frequencies=get_frequent_item_sets(dataf,0.05)

frequencies.sort_values(by=['support'], ascending=False).head(10)

"""# step 4"""

def get_arules(df,min_confident):
  sup= pd.Series(data=df.support.values, index=df['items']).to_dict()
  data=[]
  L=df['items'].values
  p=list(permutations(L,2))
  for i in p:
    if set(i[0]).issubset(i[1]):
      conf=sup[i[1]]/sup[i[0]]
      if conf > min_confident:
        j=i[1][not i[1].index(i[0][0])]
        lift= sup[i[1]]/(sup[i[0]]*sup[(j,)])
        data.append([i[0],(j,),sup[i[0]],sup[(j,)],sup[i[1]],conf,lift])
  result=pd.DataFrame(data,columns=["antecedents","consequents","antecedent support","consequent support", "support","confident","lift"])
  return(result)

#sup = pd.Series(frequencies.support.values, index=frequencies['items']).to_dict()

#type(frequencies['items'].values)

association_rules=get_arules(frequencies, 0.2)

association_rules.sort_values(by=['lift'], ascending=False)

"""# step 5

# استخراج قوانین بسیار قدرتمند
"""

frequent_items=get_frequent_item_sets(dataf,0.5)
a_rules=get_arules(frequencies, 0.5)

a_rules

"""### قوانین تولید شده در بالا با در نظر گرفتن کانفیدنت کمینه 0.5 و ساپوریت حداقل 0.5 به دست آمده اند یعنی بسیار قوانین قدرتمندی هستند همانطور که ملاحضه میشود ضریب لیفت آن ها نیز به همین نسبت بسیار بزرگتر از 1 میباشد

# step 6

## مشکل
### هنگام محاسبه ساپورت از یک مجموعه آیتم باید تمامی رکوردهای پایگاه داده را اسکن و مقایسه کرد.به طور کلی مقیاس پایگاه داده بسیار بزرگ خواهد بود که هزینه ورودی/خروجی سیستم را بسیار افزایش می دهد.

### در هر مرحله، هنگام تولید مجموعه آیتم کاندید، ترکیب‌های زیادی توسط چرخه ایجاد می‌شود، و عناصری که نباید در ترکیب دخیل باشند، مستثنی نمیشوند

## راهکار
### 1
### اگر یک ایتم شامل هیچ یک از کا عنصر نخسب یک مجموعه نباشد غیر ممکن است شامل هیچ یک از ایتم ست های کا بعلاوه یک نیز باشد پس نیازی به چک کردن نیست
### 2 
### کاهش تعداد ایتم ست های کاندید اگر دیدیم که شمارش یک کاندید از حد مینیمم فراتر رفته دیگر نیاز به شمارش اضافی آن نیست و مستقیم به مرحله بعد الگوریتم منتقلش کنیم
### 3
### میتوان در حد مرحله در هر ایتم ست به طئل
### k-1
### هر المان را شمرد و اگر تعداد ان المان از  این مقدار کمتر شد ایتم ست را حذف کرد
## راهکارهای دیگر
## 1 بازنویسی الگوریتم برمبنای ماتریس
## 2 بازنویسی الگوریتم بر مبنای ماتریس و وزن دادن

# creating candidates:
"""

def createCDDSet(dataSet):
    C=[ ]
    for tid in dataSet:
        for item in tid:
            if not [item] in C:
                C.append([item])
    C.sort()
    return list(map(frozenset,C))

"""# frozenset برای جلوگیری از تغیر ایتم ست ها استفاده میشود به طوریکه هر ایتم ست مکرر همواره از دیتاست فیلتر شود.

# اسکن و ساخت ایتم ست های کاندید
"""

def scanD(dataSet,CK,minsupport,numItem,k=0):#Accept candidate k itemsets and output frequent k itemsets
    ssCnt={}
    for tid in dataSet:
        for can in CK:
            if can.issubset(tid):
                if not  can in ssCnt :ssCnt[can] =1
                else:ssCnt[can] +=1
    #numItem=float(len(dataSet)) #Move into apriori method to avoid multiple calculations and increase complexity
    retList=[]
    supportData={}
    for key in ssCnt:
        support=float(ssCnt[key]/numItem)
        if support >= minsupport:
            retList.insert(0,key)

        # Scan D again and delete the infrequent k itemset. This improvement is to compress the transaction database and reduce the scanned data
        else:
            
            for tid in dataSet:
                if key==tid:   dataSet.remove(tid)

        supportData[key] =support

    R_List=[]
    # For frequent K itemsets, if a single item i appears less than k times, i cannot appear in the frequent k+1 itemset. Items containing a single i should be deleted from the frequent K itemset and then linked
    # Improvement direction: compression candidate set CK
    if k > 1:
        ssCnt = {}
        for tid in retList:
            for key in tid:
                if not key in ssCnt:
                    ssCnt[key] = 1
                else:
                    ssCnt[key] += 1
        tids = []
        for tid in retList:
            for item in tid:
                if item in ssCnt.keys():
                    if ssCnt[item] < k:
                        tids.append(tid)
        R_List = list(set(retList) - set(tids))





    

    #print('Frequent itemsets before optimization'+str(retList)+'              '+'Optimized frequent itemsets'+str(R_List))
    return retList,supportData,R_List

"""# بدنه اصلی الگوریتم"""

def aprioriGen(LK,k,RK):#Create candidate item set CK, where k is the number of elements in the output set

    if RK:
        LK=RK
    else:
        pass

    retList=[]
    lenLK=len(LK)
    for i in range(lenLK):
        for j in range(i+1,lenLK):
            L1= list(LK[i])[:k-2]
            L2=list(LK[j])[:k-2]
            L1.sort()
            L2.sort()
            if L1==L2:
                retList.append(LK[i]|LK[j])
    return retList

def apriori(dataSet,minsupport):
    C1=createCDDSet(dataSet)
    D=set()
    for tid in dataSet:
        tid=frozenset(tid)
        D.add(tid)
    numItem = float(len(D))  #This is the only numItem to be calculated. Otherwise, the element of data list D will be deleted by scanD method, resulting in the change of numItem
    L1,supportData,R1 =scanD(D,C1,minsupport,numItem)
    L=[L1]
    R=[R1]
    k=2
    while (len(L[k-2])>0):
        CK=aprioriGen(L[k-2],k,R[k-2])
        LK,supK,RK=scanD(D,CK,minsupport,numItem,k)
        supportData.update(supK)
        L.append(LK)
        R.append(RK)
        k +=1
    L = [i for i in L if i]#Delete empty list
    ss=list(supportData.items())
    ssd=pd.DataFrame(ss)
    ll=pd.DataFrame(L)

    return ll,ssd

dtl,dts=apriori(dataf,0.2)

"""## frequent itemsets"""

dtl

"""## supports"""

dts